\chapter{Experiments}
\label{ch:experiments}

This chapter details the experiments performed on the systems described previously, contemplating since the front-end processes until the speaker modeling and the log-likelihood ratio test (see \equationref{score_of_X}). First, a description of the corpus is made. Then, an explanation about the implementation is given. At last, the results using the feature extraction process and the GMM techniques are exhibited.

\section{Corpus}
\label{sec:corpus}

The database used is the MIT Mobile Device Speaker Verification Corpus (MIT-MDSCV), \refbib{Woo et. al.}{woo.park.hazen.2006}, a \textbf{corpus} designed to evaluate voice biometric systems of high mobility. All utterances were recorded using mobile devices of different models and manufacturers, to avoid issues related to a specific device. Also, the recordings were performed in actual noisy environments, presenting the Lombard effect (i.e., speakers alter their style of speech in noisier conditions in an attempt to improve intelligibility).

The corpus is composed of three sessions. The first, named ``Enroll 1", contains 48 speakers (22 females and 26 males), each with 54 utterances (names of ice cream flavors) of 1.8 seconds average duration, and is used to train an ASR system. The utterances were recorded in three different locations (a quiet office, a mildly noisy hallway, and a busy street intersection) as well as two different microphones (the built-in internal microphone of the handheld device and an external earpiece headset) leading to 6 distinct training conditions equally dividing the session. The second, named ``Enroll 2", is similar to the first with a difference in the order of the spoken utterances, and is used to test the enrolled speakers. The third session, named ``Imposters", is similar to the first two, but with 40 non-enrolled speakers (17 females and 23 males), and is used to test the robustness of the ASR system (imposters should be rejected). The table below summarizes the division of the corpus.

\begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|c|c|}
    \hline
    \multicolumn{1}{|c|}{{\bf Session}} & {\bf Training} & {\bf Test} & {\bf \#female} & {\bf \#male} \\ \hline
    Enroll 1 & {\bf X} & {\bf } & 22 & 26 \\ \hline
    Enroll 2 & {\bf } & {\bf X} & 22 & 26 \\ \hline
    Imposter & {\bf } & {\bf X} & 17 & 23 \\ \hline
    \end{tabular}
    \caption{Corpus divided in training and test sets.}
    \label{tab:corpus-division}
\end{table}

All utterances are recorded in uncompressed WAV files using a single channel. For each utterance record there is a correspondent text file containing pertinent information, such as speaker, microphone, location, message content and etc. (see \tableref{utterance-info}).

Despite being a base for speaker verification systems, in this report MIT-MDSCV is also used for speaker identification experiments. The difference is that only ``Enroll 1" and ``Enroll 2" are used, for training and test respectively. In an ideal identification system all utterances from ``Enroll 2" are correctly identified and in an ideal verification system the false detection and false rejection rates are zero.

\begin{table}[h]
    \centering
    \begin{tabular}{|l|c|}
    \hline
    {\bf Speaker}    & f00              \\ \hline
    {\bf Session}    & 1                \\ \hline
    {\bf List}       & female\_list\_3a \\ \hline
    {\bf Gender}     & female           \\ \hline
    {\bf Location}   & Office           \\ \hline
    {\bf Microphone} & Headset          \\ \hline
    {\bf Phrase}     & karen livescu    \\ \hline
    \end{tabular}
    \caption{Information from first utterance of first speaker in session ``Enroll 1".}
    \label{tab:utterance-info}
\end{table}

\section{Coding and Data Preparation}
\label{sec:coding-and-data-preparation}

The systems described throughout the report were implemented in the Python programming language, version 3.4.3, and the frameworks NumPy 1.8.1 and SciPy 0.14.0 were used to perform most of the calculations as matrices. Also, Matplotlib 1.4 was used to plot the results as figures. All codes can be found in \url{github.com/embatbr/tg} and are free to be used, since properly referenced.

The implementation is divided in 6 modules. Module \textbf{features} contains codes to execute the feature extraction through the MFCC algorithm. Each stage is executed by a different function. The main function joins all stages, receiving a speech signal in waveform and delivering a matrix of features over frames. Most of this module was written using codes from \refbib{Lyons}{lyons.2012}. The GMM is implemented in module \textbf{mixtures} as a class containing methods for training usign EM algorithm and for bayesian adaptation. The module also contains functions to execute the \emph{k-means} clustering, used during the GMM creation. Functions to extract the features from MIT-MDSCV and to read groups of feature data (e.g., single speaker and background) are present in module \textbf{bases}. The module \textbf{main}, as the name denounces, is filled with functions to execute every functionality needed. Through command lines it is possible to extract MFCCs from the base, train and adapt models, identify or verify speakers, calculate rates, draw curves and etc. Module \textbf{show} is aimed to generate figures to fill the previous chapters (mostly chapters \chapterrefcomp{feature-extraction} and \chapterrefcomp{gmm}). At last, module \textbf{common} contains useful constants and functions shared by the other modules.

\subsection{Parameters}

The MFCCs extracted used a filterbank of size 26, maintaining only the first 19 features, and deltas of order 0, 1 and 2, leading to MFCCs with final feature number of 19, 38 and 57, respectively. The deltas were calculated using $K = 2$. Before any delta calculation, the energy appending and CMS steps were performed. The relevance factor $r$ in AGMM had a default value of 16 for all combinations of adaptations. The implemented EM algorithm stops the training when $\log\postpdf{\dvec{X}}{\lambda^{(k+1)}} - \log\postpdf{\dvec{X}}{\lambda^{(k)}} \le 10^{-3}$.

All speakers were modeled using SSGMMs with sizes ($M$s) equal to 8, 16, 32, 64 and 128, each divided in 4 types of environment configuration: quiet office, mildly noisy hallway, busy street intersection and all three together. This leads to 48 speakers $\times$ 5 $M$s $\times$ 4 environments $\times$ 3 delta orders $=$ 2880 trained SSGMMs. The UBM is a combination of a trained male UBM and a trained female UBM (see \figureref{ubm-diagram}b), for all sizes of mixtures, types of environments and orders of delta, totaling 60 models. Each one of the 4 combinations of SSAGMM also contains 2880 models. Adding the Single Speaker Fractional Gaussian Mixture Models (SSFGMMs) for all 5 values of $r$ (0.95, 0.99, 1, 1.01 and 1.05), the total number of trained models is 28860.

\subsection{Algorithmic Issues}

\subsubsection{Initialization:}

According to \refbib{Reynolds}{reynolds.1995c}, different methods of initialization lead to different local maxima reached by the EM algorithm. Following the approach used in the referenced paper, in this work the models are generated randomly selecting the means and executing a single iteration \emph{k-means} to initialize the means, nodal variances and weights. A complete k-means (iterate until all clusters remain unchanged) during the initialization, followed by EM training, leads to a similar result, making the choice of single iteration k-means in the initialization the logical decision.

\subsubsection{Variances:}

During the M-step of the EM algorithm, some variances may have their values decreased significantly. This represents a degeneration of the model, occuring when the size of mixture increases more than needed to represent the data or when the data is insufficient to train the model, \refbib{Reynolds}{reynolds.1995c}. This issue occurred in SSGMMs and SSFGMMs with 64 or 128 gaussians and UBMs with 128 gaussians, trained with utterances from a single environment. To prevent the degeneration, the EM algorithm receives a constraint:

\begin{equation}
    \sigma^2 < \sigma_{min}^2 \implies \sigma^2 \gets \sigma_{min}^2,
    \label{eq:variance-prune}
\end{equation}

\noindent for $\sigma_{min}^2 = 0.01$. This test is made in every iteration and provides more robustness to the ASR systems (correct the degradation when it occurs, avoiding error propagation).

\subsubsection{Monotonicity:}

The Expectation-Maximization is a monotonic algorithm. For every iteration $k$, the rule $\log\postpdf{\dvec{X}}{\lambda^{(k+1)}} \geq \log\postpdf{\dvec{X}}{\lambda^{k}}$ is always true. The algorithm stops only when a local maximum is reached (the likelihood remains stable) or when the increase is smaller than a defined threshold and there is no practical gain in continue. This rule is not true for an EM performed by a FGMM. The 4 non-unitary values of $r$ presented this discrepancy, more frequent with the increase in distance to 1. When $\log\postpdf{\dvec{X}}{\lambda^{(k+1)}} < \log\postpdf{\dvec{X}}{\lambda^{k}}$ occurs, the stop condition is satisfied and the EM returns the last calculated likelihood.

\section{Experiments and Results}
\label{sec:experiments-and-results}

Using the trained models, two types of experiments were performed. The first, speaker identification, was executed using SSGMMs and SSFGMMs. Identification through SSGMM is an experiment known since \refbib{Reynolds}{reynolds.1992} was published in 1992, with countless revisions, improvements and derivations. Conversely, identification through SSFGMM is a technique never tried before and for that reason FGMM was used in speaker identification to validate the idea before further experiments. At the end SSGMM and SSFGMM were compared. The second type, speaker verification, was executed using SSGMMs and SSAGMMs, providing false detection and false rejection rates. The verification through SSAGMM was executed using only combinations of adaptations that produced curves with a behavior similar to the verification through SSGMM (see the previous section). Finally, a comparison between SSGMM and SSAGMM was performed.

The metrics used for evaluation differ according to the type of ASR system studied. For identification the interest is to know how well the system correctly identify an enrolled user. The logical way is by analyzing the success rate (the closer to 100\% the better). For verification, the major concern for the system designer is to avoid misclassification. An imposter can be considered an enrolled speaker (false detection), and an enrolled speaker can be considered an imposter (false rejection). To evaluate the correctness of a system, a Detection Error Tradeoff (DET) curve is used, \refbib{Martin et al.}{martin.et.al.1997}. The perfect point of operation is when false detection and false rejection rates are equal, a measure named Equal Error Rate (EER). EER is used as a starting point to create more restrictive or permissive systems.

In the experiments performed, all 54 utterances from each speaker were tested against any one of the models created and trained for each environment configuration. The first, \textbf{office}, is practically a noise free space. The second, \textbf{hallway}, presents a moderate level of noise. The third, \textbf{intersection}, is a very lound environment. The last is a combination of all three previous environments, providing a model trained with different levels of noise, and thus, more robust. This approach allows a more refined analysis of the system's response to background noise.

\subsection{Speaker Identification using SSGMM}

As shown in \equationref{speaker_identification}, given a set $\dvec{\lambda} = \{\lambda_1, ..., \lambda_S\}$ of SSGMMs, a sequence of features $\boldsymbol{X}$ is assigned to a speaker $\mathcal{S}_i$, represented by a model $\lambda_i$ of $\dvec{\lambda}$, if $\postpdf{\boldsymbol{X}}{\lambda_i}$ is greater than all $\postpdf{\boldsymbol{X}}{\lambda_j}$, for $\lambda_j \in \dvec{\lambda}$ and $j \ne i$.

\input{chapters/tables/identify_speakers}

In this study \equationref{speaker_identification} is used for all 54 speech signals of each enrolled speaker from session ``Enroll 2". The number of tests is 54 utterances $\times$ 48 enrolled speakers $\times$ 4 environment configurations $\times$ 5 sizes of mixture, totaling 51840. Identities are attributed regardless of gender (a female speaker may be identified as male and vice-versa).

As seen in the table above and in \figureref{identification-curves-speakers}, for the fourth environment configuration, named \emph{all}, the success rates increase with $M$, while for \emph{office} they reach a maximum value at $M = 64$, and for \emph{hallway} and \emph{intersection}, at $M = 32$. As explained in the previous section, SSGMMs suffered degeneration due to a decrease in the values of variances for 64 and 128 gaussians. A higher value of $M$ led to higher misrepresentation of the samples by the SSGMM. The exception to this ``rule" occurs when all environments are used in the training, that may be explained due to the higher number of samples (each speaker has 18 utterances recorded in each environment, leading to all 54 speech signals being used to train the model). Also as expected, when all environments are present, the SSGMM represents different levels of background noise, what produces a more robust modeling.

\begin{figure}[ht]
    \centering
    \includegraphics{chapters/experiments/identification-curves-speakers}
    \caption{Identification rates for enrolled speakers.}
    \label{fig:identification-curves-speakers}
\end{figure}

\subsection{Speaker Identification using SSFGMM}

The only difference between this experiment and the previous is the type of GMM used. In traditional speaker identification, each speaker $\mathcal{S}_j$ is represented by a $\lambda_j$, using the GMM defined in \sectionref{gmm-definition} and the EM algorithm described in \sectionref{em}. When FCM is used to model the covariance matrix, the SSGMM is replaced by SSFGMM with values 0.95, 0.99, 1.00, 1.01 and 1.05 for $r$, maintaining the rest of the experiment unchanged.

The choice of values for $r$ was made using

\begin{equation}
    r = r_0 + (-1)^u\delta,
    \label{eq:choice-of-r}
\end{equation}

\noindent where $r_0 = 1$, $u \in \{0, 1\}$ and $\delta \in \{0.01, 0.05\}$, providing $r$s higher and lower than $r_0$ to conduct a primary study of which values increase the success rates. \appendixref{results-identify-ssfgmm} contains all tables and figures showing the results for this experiment.

A comparison of \tableref{identify_speakers} and \figureref{identification-curves-speakers} with the tables and figures from \appendixref{results-identify-ssfgmm} indicates that SSFGMMs trained using utterances from all environments and values 0.99, 1 and 1.01 for $r$ present similar speaker identification success rates to the experiment using SSGMM. The differences are around 1 percentage point, what may not be characterized as an improvement before further investigations (a slightly better success rate may be attributed to the random initialization of the model). For $r$ equals to 0.95 and 1.05, the success rates are near to 20 percentage points lower. A value lower than 1 decreases the variances, approaching zero when $r \to 0$, while a value higher than 1 increases the variance (with the deviation passing the data limits) and sets all means to the same point, as seen in the figure below. Also, the EM algorithm violates the convergence rule faster (1 or 2 iterations for the values of $r$ in the figure).

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{chapters/experiments/frac-em-extremes}
    \caption{GMM ``trained" using FCM, with $r = 0.8$ (left) and $r = 1.2$ (right). Only the first deviation is shown.}
    \label{fig:frac-em-extremes}
\end{figure}

\subsection{Speaker Verification using SSGMM}

The speaker verification through SSGMM is performed by \equationref{score_of_X} for models $\lambda_C = \lambda_{SSGMM}$ and $\lambda_{bkg} = \lambda_{UBM}$, and a sequence $\dvec{X}$ of features. Similarly to the speaker identification experiments, all 54 speech signals from each speaker are tested for systems trained in the four environment configurations. The difference is the test of imposters. Then, an enrolled speaker's model is tested for all his or her utterances and all utterances from the imposters (with the same gender).

\input{chapters/tables/verify_speakers}

\begin{figure}[ht]
    \centering
    \includegraphics{chapters/experiments/verification-eer-speakers}
    \caption{Verification EERs over $M$ for enrolled speakers.}
    \label{fig:verification-eer-speakers}
\end{figure}

Similarly to the identification experiments, systems trained with utterances from all environments present better performance (the ideal EER is zero), due to a superior robustness to background noise. \figureref{verification-eer-speakers} shows a performance drop when $M$ value is 64 and 128 for systems trained in separated environments. Such as the previous experiments, the degradation of the model in face of insufficient data is the most logical explanation. The DET curves for all 5 values of $M$ are shown in \appendixref{results-verify} (along with the table and figure showed above).

\subsection{Speaker Verification using SSAGMM}

The verification through SSAGMM is similar to the previous, differing on $\lambda_C$ not being trained, but adapted from a trained UBM. The adaptation process is constant in algorithmic time, different from the Expectation-Maximization, that remains in a loop until a convergence is reached. \appendixref{results-verify} provides tables and figures showing the results for all experiments that adapt the means, alone or combined with adaptations of weights and/or variances. \tableref{rank-verify-tables} shows the values of EER for each combination trained with utterances from all environments.

\input{chapters/tables/rank-verify-tables}

The EERs shown are for models trained using features recorded in all environments (the design choice for a real world system) and decrease for higher values of $M$. Although the EERs shown for speaker verification using SSGMM are better, the differences when using higher sizes of mixtures are small enough to consider the replacement of SSGMM by SSAGMM. The natural choice is to choose the systems with the means and variances adapted (referenced as \emph{mv-systems}), due to the higher performance in most of the cases (see \tableref{rank-verify-tables}), followed closely by the systems that adapt all parameters (referenced as \emph{wmv-systems}), that outperform the mv-systems in some cases. The differences of EERs from mv-systems and wmv-systems are marginal, and the system designer may consider the choice of any one of the two. This question may be decided conducting experiments with values of $M$ higher than 128, providing more tools to make a better decision. The other two combinations (i.e., \emph{m-systems} and \emph{wm-systems}) performance are always worse than the first two.