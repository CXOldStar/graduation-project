\chapter{Experiments}
\label{ch:experiments}

This chapter details the experiments performed on the systems described in the previous chapters, contemplating from the front-end processes until the speaker modeling and the log-likelihood ratio test (see \equationref{score_of_X}). First, a description of the corpus is made. Then, explanations about the implementation are given. At last, the results are exhibited using the feature extraction process and the GMM techniques.

\section{Corpus}
\label{sec:corpus}

The database used in this work is the MIT Mobile Device Speaker Verification Corpus (MIT-MDSCV), \refbib{Woo et. al.}{woo.park.hazen.2006}, a \textbf{corpus} designed to evaluate voice biometric systems of high mobility. All utterances were recorded using mobile devices of different models and manufacturers, to avoid issues related to a specific device. Also, the recordings were performed in actual noisy environments, presenting the Lombard effect (i.e., speakers alter their style of speech in noisier conditions in an attempt to improve intelligibility).

The corpus is composed of three sessions. The first, named ``Enroll 1", contains 48 speakers (22 females and 26 males), each with 54 utterances (names of ice cream flavors) of 1.8 seconds average duration, and is used to train an ASR system. The utterances were recorded in three different locations (a quiet office, a mildly noisy hallway, and a busy street intersection) as well as two different microphones (the built-in internal microphone of the handheld device and an external earpiece headset) leading to 6 distinct training conditions equally dividing the session. The second, named ``Enroll 2", is similar to the first with a difference in the order of the spoken utterances, and is used to test the enrolled speakers. The third session, named ``Imposters", is similar to the first two, but with 40 non-enrolled speakers (17 females and 23 males), and is used to test the robustness of the ASR system (imposters should be rejected). \tableref{corpus-division} summarizes the division of the corpus.

\begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|}
    \hline
    \multicolumn{1}{|c|}{{\bf Session}} & {\bf Training} & {\bf Test} \\ \hline
    Enroll 1                            & {\bf X}        & {\bf }     \\ \hline
    Enroll 2                            & {\bf }         & {\bf X}    \\ \hline
    Imposter                            & {\bf }         & {\bf X}    \\ \hline
    \end{tabular}
    \caption{Corpus divided in training and test sets.}
    \label{tab:corpus-division}
\end{table}

All utterances are recorded in uncompressed WAV files using a single channel. For each utterance record there is a correspondent text file containing pertinent information, such as speaker who produced it, microphone used, location where the utterance was recorded, the spoken message content and etc. (see \tableref{utterance-info}).

Despite being a base for speaker verification systems, in this paper MIT-MDSCV is also used for speaker identification experiments. The difference is that only ``Enroll 1" and ``Enroll 2" are used, for training and test respectively. In an ideal identification system all utterances from ``Enroll 2" are correctly identified and in an ideal verification system the false detection and false rejection rates are zero.

\begin{table}[h]
    \centering
    \begin{tabular}{|l|c|}
    \hline
    {\bf Speaker}    & f00              \\ \hline
    {\bf Session}    & 1                \\ \hline
    {\bf List}       & female\_list\_3a \\ \hline
    {\bf Gender}     & female           \\ \hline
    {\bf Location}   & Office           \\ \hline
    {\bf Microphone} & Headset          \\ \hline
    {\bf Phrase}     & karen livescu    \\ \hline
    \end{tabular}
    \caption{Information from first utterance of first speaker in session ``Enroll 1".}
    \label{tab:utterance-info}
\end{table}

\section{Coding and Data Preparation}
\label{sec:coding-and-data-preparation}

The systems described throughout the paper were implemented in the Python programming language, version 3.4.3, and the frameworks NumPy 1.8.1 and SciPy 0.14.0 were used to perform most of the calculations as matrices. Also, Matplotlib 1.4 was used to plot the results as figures. All codes can be found in \url{github.com/embatbr/tg} and are free to be used, since properly referenced.

The implementation is divided in 6 modules. Module \textbf{features} contains codes to execute the feature extraction through the MFCC algorithm (see \chapterref{feature-extraction}). Each stage is executed by a different function. The main function joins all stages, receiving a speech signal in waveform and delivering a matrix of features over frames. Most of this module was written using codes from \refbib{Lyons}{lyons.2012}. The GMM is implemented in module \textbf{mixtures} as a class containing methods for training usign EM algorithm and for bayesian adaptation (see \sectionref{adapted-gmm}). The module also contains functions to execute the \emph{k-means} clustering, used during the GMM creation. Functions to extract the features from MIT-MDSCV and to read groups of feature data (e.g., single speaker and background) are present in module \textbf{bases}. The module \textbf{main}, as the name denounces, is filled with functions to execute every functionality needed. Through command lines it is possible to extract MFCCs from the base, train and adapt models, identify or verify speakers, calculate rates and draw curves and etc. Module \textbf{show} is aimed to generate figures to fill the previous chapters (mostly chapters \chapterrefcomp{feature-extraction} and \chapterrefcomp{gmm}). At last, module \textbf{common} contains useful constants and functions shared by the other modules.

\subsection{Parameters}

The MFCCs extracted used a filterbank of size 26, maintaining only the first 19 features, and deltas of order 0, 1 and 2, leading to MFCCs with final feature number of 19, 38 and 57, respectively. The deltas were calculated using $K = 2$. Before any delta calculation take place, the energy appending and CMS steps are performed. The relevance factor $r$ in AGMM has a default value of 16 for all combinations of adaptations. The EM algorithm stops the training when $\log\postpdf{\dvec{X}}{\lambda^{(k+1)}} - \log\postpdf{\dvec{X}}{\lambda^{(k)}} \le 10^{-3}$.

All speakers are modeled using SSGMMs with sizes ($M$s) 8, 16, 32, 64 and 128, each divided in 4 types of environment configuration: quiet office, mildly noisy hallway, busy street intersection and all three together. This leads to 48 speakers $\times$ 5 $M$s $\times$ 4 environments $\times$ 3 delta orders $=$ 2880 trained SSGMMs. The UBM is a combination of a trained male UBM and a trained female UBM (see \figureref{ubm-diagram}b), for all sizes of mixtures, types of environments and orders of delta, totaling 60 models. Each one of the 7 combinations of SSAGMM also contains 2880 models. Adding the Single Speaker Fractional Gaussian Mixture Models (SSFGMMs) for all 5 values of $r$ (1, 0.99, 1.01, 0.95 and 1.05), the total number of trained models is 37500.

\subsection{Algorithmic Issues}

\subsubsection{Initialization:}

According to \refbib{Reynolds}{reynolds.1995c}, different methods of initialization lead to different local maxima reached by the EM algorithm. Following the approach used in the referenced paper, in this work the models are generated randomly selecting the means and executing a single iteration \emph{k-means} to initialize the means, nodal variances and weights. A complete k-means (iterate until all clusters remain unchanged) during the initialization, followed by EM training, leads to a similar result, making the choice of single iteration k-means in the initialization the logical decision.

\subsubsection{Variances:}

During the M-step of the EM algorithm, some variances may have their values decreased significantly. This represents a degeneration of the model, occuring when the size of mixture increases more than needed to represent the data or when the data is insufficient to train the model, \refbib{Reynolds}{reynolds.1995c}. This issue occurred in SSGMMs with 64 or 128 gaussians and UBMs with 128 gaussians. To prevent the degeneration, the EM algorithm receives a constraint: when the variance $\sigma^2$ is lower than a minimum variance $\sigma_{min}^2 = 0.01$, $\sigma_{min}^2$ is assigned to $\sigma^2$. This test is made in every iteration and provides more robustness to the ASR system (correct the degradation when it occurs, avoiding error propagation).

\section{Experiments and Results}
\label{sec:experiments-and-results}

Using the trained models (see previous section), two types of experiments were performed. The first, speaker identification in open set (see \sectionref{speaker-identification}), was executed using SSGMMs and SSFGMMs. Identification through SSGMM is an experiment known since \refbib{Reynolds}{reynolds.1992} was published in 1992, with countless revisions proving its correctness. Conversely, identification through SSFGMM (see \sectionref{frac-gmm}) is a technique never tried before and for that reason FGMM was used in speaker identification to validate the idea before further experiments. At the end SSGMM and SSFGMM were compared. The second type, speaker verification, was executed using SSGMMs and SSAGMMs, providing false detection and false rejection rates. The verification through SSAGMM was executed using all combinations of adaptations. Finally, a comparison between SSGMM and SSAGMM was performed.

The metrics used for evaluation differ according to the type of ASR system studied. For identification in closed set the interest is in know how well the system correctly identify an enrolled user. The logical way is by analyzing the success rate (the closer to 100\% the better). For verification, the major concern for the system designer is to avoid misclassification. The perfect point of operation is when false detection and false rejection rates are equal (Equal Error Rate, EER), used to evaluate the correctness of the system.

In the experiments performed, all 54 utterances from each speaker were tested against any one of the models created and trained for each environment configuration. The first, \textbf{office}, is a noise free environment. The second, \textbf{hallway}, presents a moderate level of noise. The third, \textbf{intersection} is a very lound environment. The last is a combination of all three previous environments, providing a model trained with different levels of noise. This approach allows a more refined analysis of the system's response to background noise.

\subsection{Speaker Identification using SSGMM}

As shown in \sectionref{speaker-identification}, \equationref{speaker_identification}, given a set $\dvec{\lambda} = \{\lambda_1, ..., \lambda_S\}$ of SSGMMs, a sequence of features $\boldsymbol{X}$ is assigned to a speaker $\mathcal{S}_i$, represented by a model $\lambda_i$ of $\dvec{\lambda}$, if $\postpdf{\boldsymbol{X}}{\lambda_i}$ is greater than all $\postpdf{\boldsymbol{X}}{\lambda_j}$, for $\lambda_j \in \dvec{\lambda}$ and $j \ne i$.

In this study, \equationref{speaker_identification} is used for all 54 speech signals of each enrolled speaker. The signals used are in session ``Enroll 2", as described in \sectionref{corpus}. The number of tests is 54 utterances $\times$ 48 enrolled speakers $\times$ 4 environment configurations $\times$ 5 sizes of mixture, totaling 51840. \tableref{identify_speakers} shows the speaker identification success rates (in percent).

\input{chapters/tables/identify_speakers}

As seen in the tables above and in \figureref{identification-curves-speakers}, for the fourth environment configuration, all, the success rates increase with $M$, while for office they reach a maximum value at $M = 64$, and for hallway and intersection, at $M = 32$. As explained in the previous section, SSGMMs suffered degeneration due to a decrease in the values of variances for 64 and 128 gaussians. A higher value of $M$ led to higher misrepresentation of the samples by the SSGMM. The exception to this ``rule" occurs when all environments are used in the training, that may be explained due to the higher number of samples (each speaker has 18 utterances recorded in each environment, leading to all 54 speech signals being used to train the model). Also as expected, when all environments are present, the SSGMM represents different levels of background noise, what produces a more robust modeling.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{chapters/experiments/identification-curves-speakers}
    \caption{Identification rates for enrolled speakers using different $\Delta$s.}
    \label{fig:identification-curves-speakers}
\end{figure}

\subsection{Speaker Identification using SSFGMM}

The only difference between this experiment and the previous is the type of GMM used. In traditional speaker identification, each speaker $\mathcal{S}_j$ is represented by a $\lambda_j$, using the GMM defined in \sectionref{gmm-definition} and the EM algorithm described in \sectionref{em}. Using FCM to model the covariance matrix, the SSGMM is replaced by SSFGMM with values 0.95, 0.99, 1.00, 1.01 and 1.05 for $r$, maintaining the rest of the experiment unchanged.

The choice of values for $r$ was made using

\begin{equation}
    r = r_0 + (-1)^u\delta,
    \label{eq:choice-of-r}
\end{equation}

\noindent where $r_0 = 1$, $u \in \{0, 1\}$ and $\delta \in \{0.01, 0.05\}$, providing $r$s higher and lower than $r_0$ to conduct a primary study of which values increase the success rates. \sectionref{results-identify-ssfgmm} contains tables and figures showing the results for this experiment.

A comparison of \tableref{identify_speakers} and \figureref{identification-curves-speakers} with the tables and figure from \sectionref{results-identify-ssfgmm} indicates that SSFGMMs trained using utterances from all environments and values 0.99, 1 and 1.01 for $r$ present similar speaker identification success rates to the experiment using SSGMM. The differences are around 1 percentage point, what may not be characterized as an improvement before further investigations (a slightly better success rate may be attributed to the random initialization of the model).

%\subsection{Speaker Verification using SSGMM}

%\subsection{Speaker Verification using SSAGMM}

%\subsection{Speaker Verification using SSFGMM}

%\subsection{Comparison between SSGMM and SSAGMM}

%\subsection{Comparison between SSGMM and SSFGMM}